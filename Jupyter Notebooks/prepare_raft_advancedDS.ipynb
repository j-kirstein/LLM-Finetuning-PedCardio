{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ce6c3-dfe4-4ee6-8551-4f9eefcbab8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q --progress-bar off --no-warn-conflicts llama-index-core llama-index-readers-docling llama-index-node-parser-docling llama-index-embeddings-huggingface llama-index-llms-huggingface-api llama-index-vector-stores-milvus llama-index-readers-file python-dotenv\n",
    "!pip install backports.tarfile\n",
    "!pip install docling --upgrade\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index\n",
    "!pip install docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dc1be-f42f-4a02-a1b7-cc8aa2ae3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hf_token = \"HUGGINGFACE TOKEN HERE\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=hf_token\n",
    "os.environ[\"HF_TOKEN\"]=hf_token\n",
    "os.environ['HF_HOME'] = 'YOUR_HOME_DIR/.cache/huggingface/'\n",
    "os.environ['TRANSFOMERS_CACHE'] = 'YOUR_HOME_DIR/.cache/huggingface/'\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e8006-b418-4a71-9455-a258f859c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from warnings import filterwarnings\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import torch\n",
    "from llama_index.core import Settings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from peft import PeftModel\n",
    "from llama_index.core import Settings\n",
    "\n",
    "EMBED_MODEL = HuggingFaceEmbedding(model_name=\"abhinand/MedEmbed-large-v0.1\")\n",
    "embed_dim = len(EMBED_MODEL.get_text_embedding(\"hi\"))\n",
    "Settings.embed_model = EMBED_MODEL\n",
    "print(\"Embed dim:\", embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fb0b0-086c-491f-a513-d8781df72df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "baseDir = \"YOUR_HOME_DIR/dataset_cpu_gpu_merge\"\n",
    "\n",
    "dataset = datasets.load_from_disk(baseDir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c287e8-44f3-4a07-ba5a-698cd45795d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from docling.document_converter import DocumentConverter\n",
    "from llama_index.node_parser.docling import DoclingNodeParser\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "SOURCE = r\"YOUR_HOME_DIR/guideline_edit.md\"\n",
    "\n",
    "reader = DoclingReader()\n",
    "node_parser = MarkdownNodeParser()\n",
    "chunker = HybridChunker()\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=str(Path(\"YOUR_HOME_DIR/datasets/docling_md_vectordb.db\")),\n",
    "    dim=embed_dim,\n",
    "    overwrite=True,\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=reader.load_data(SOURCE),\n",
    "    transformations=[node_parser],\n",
    "    storage_context=storage_context,\n",
    "    embed_model=EMBED_MODEL,\n",
    ")\n",
    "\n",
    "storage_context.persist(persist_dir=\"YOUR_HOME_DIR/datasets/persistent_vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958a930-d78e-418b-b576-554e2e94d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareForRaft():\n",
    "    for e in dataset:\n",
    "        query = e[\"question\"]\n",
    "        \n",
    "        retriever = index.as_retriever(similarity_top_k=5)\n",
    "        \n",
    "        retrieved_docs = retriever.retrieve(query)\n",
    "\n",
    "        \n",
    "        sources = [s.get_content(s.metadata) for s in retrieved_docs]\n",
    "        sourcesStr = \"\\n\\n\".join(sources)\n",
    "        e[\"sources\"] = sources\n",
    "        newQ = \"Question:\\n\" + query + \"\\n\\nContext:\\n\" + sourcesStr\n",
    "        e[\"input\"] = newQ\n",
    "\n",
    "        if includeAnswerThoughts:\n",
    "            newAns = \"<thoughts>\\n\" + e[\"answerThoughts\"] + \"</thoughts>\\n\" + e[\"answer\"]\n",
    "            e[\"answer\"] = newAns\n",
    "\n",
    "        del e[\"initialThoughts\"]\n",
    "        del e[\"isDuplicate\"]\n",
    "        del e[\"isAnswerable\"]\n",
    "        del e[\"answerThoughts\"]\n",
    "    \n",
    "        yield e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3aa25bd-cfa2-44c3-892f-09c91ee9e233",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "includeAnswerThoughts=False\n",
    "newDS = Dataset.from_generator(prepareForRaft)\n",
    "example = newDS[0]\n",
    "for k in example.keys():\n",
    "    print(f\"{k}:\")\n",
    "    print(example[k])\n",
    "\n",
    "print(\"Length documents: \", len(example[\"sources\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf88d1-021f-45c2-b792-b88da9d704a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "import os\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "test_valid = newDS.train_test_split(test_size=0.15, seed=seed)\n",
    "\n",
    "convDSPath = \"YOUR_HOME_DIR/datasets/deepseek_noCOT_raft/\"\n",
    "\n",
    "if not os.path.exists(convDSPath):\n",
    "    os.mkdir(convDSPath)\n",
    "\n",
    "test_valid['train'].to_parquet(convDSPath + \"train.parquet\")\n",
    "test_valid['test'].to_parquet(convDSPath + \"val.parquet\")\n",
    "\n",
    "print(\"Train Length:\", len(test_valid['train']))\n",
    "print(\"Val Length:\", len(test_valid['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc561813-2d35-4269-bb8e-931420a3303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad7337c-7101-4cdc-b18e-22cb3eb98dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
