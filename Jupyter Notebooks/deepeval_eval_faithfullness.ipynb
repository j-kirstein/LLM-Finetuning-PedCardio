{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cae237-2bd9-4b9d-98a7-e2e7d62565c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import FaithfulnessMetric\n",
    "import deepeval\n",
    "\n",
    "import os\n",
    "openai_token = \"YOUR OPENAI TOKEN\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=openai_token\n",
    "\n",
    "deepeval.login_with_confident_api_key(\"YOUR DEEPEVAL TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc6ea2-de18-47d0-ac9a-a2963ff87816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def remove_thoughts_section(text):\n",
    "    return flatten([e.split(\"</think>\") for e in text.split(\"</thoughts>\")])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90297795-7e29-45f9-a4da-476a189911cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from deepeval import evaluate\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "faithfulness_metric = FaithfulnessMetric(\n",
    "    include_reason=True,\n",
    "    #model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "\n",
    "usedDataset = \"deepseek_COT_raft\"\n",
    "dsBaseDir = f\"YOUR_HOME_DIR/datasets/evaluation/{usedDataset}/\"\n",
    "\n",
    "models = [f for f in os.listdir(dsBaseDir) if os.path.isdir(dsBaseDir + f)]\n",
    "\n",
    "allResults = {}\n",
    "\n",
    "IncludeThoughts = False\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1c9a4e-7856-4003-a075-ee38af453b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['qwen-32b-basicDS', 'meditron-7b-fp16', 'meditron-7b-basicDS', 'llama31-basicDS', 'llama31-advDS-COT', 'llama3.1-8b-instruct-fp16', 'qwen-32b-COT-q4_K_M']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b3846-a094-4d9d-ba74-a66fbe18e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "usedModel = models[4]\n",
    "print(\"Evaluating Faithfullness for\", usedModel)\n",
    "datasetDir = dsBaseDir + usedModel\n",
    "\n",
    "ds = Dataset.load_from_disk(datasetDir)\n",
    "\n",
    "testcases = []\n",
    "for e in ds:\n",
    "    if IncludeThoughts:\n",
    "        actual_output=e[\"actual_output\"]\n",
    "        expected_output=e[\"expected_output\"]\n",
    "    else:\n",
    "        actual_output=remove_thoughts_section(e[\"actual_output\"])\n",
    "        expected_output=remove_thoughts_section(e[\"expected_output\"])\n",
    "    test_case = LLMTestCase(input=e[\"input\"], actual_output=actual_output, expected_output=expected_output, retrieval_context=e[\"retrieval_context\"])\n",
    "    testcases.append(test_case)\n",
    "id = f\"new-{usedModel}-{usedDataset}-{'WithThoughts' if IncludeThoughts else 'WithoutThoughts'}\"\n",
    "dataset = EvaluationDataset(test_cases=testcases)\n",
    "\n",
    "try:\n",
    "    evalRes = evaluate(dataset, metrics=[faithfulness_metric], throttle_value=30, identifier=id, print_results=False)\n",
    "    allResults[usedModel] = evalRes\n",
    "except:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2c90e-7a5e-4311-8eba-0c44fcf3d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "#models = []\n",
    "#for model_name in allResults.keys():\n",
    "#    m = model_name\n",
    "#    e = allResults[m]\n",
    "#    scores = []\n",
    "#    for r in e.test_results:\n",
    "#        scores.append(r.metrics_data[0].score)\n",
    "#    avg = round(statistics.fmean(scores), 2)\n",
    "#    print(model_name)\n",
    "#    print(\"Avg:\", avg)\n",
    "#    models[m] = avg\n",
    "\n",
    "#print(models)\n",
    "# Extracting values\n",
    "models = {'llama3.1-8b-instruct-fp16':0.96, 'meditron-7b-fp16':0.99, 'DeepSeek-R1-Distill-Qwen-32B':0.94, 'llama31-basicDS':0.98, 'meditron-7b-basicDS':0.95, 'qwen-32b-basicDS':0.94, 'llama31-advDS-COT':0.94, 'meditron-7b-COT':0.94, 'qwen-32b-COT-q4_K_M':0.95}\n",
    "colors = ['#1b9e77'] * 3 + ['#d95f02'] * 3 + ['#7570b3'] * 3\n",
    "\n",
    "print(models)\n",
    "avgs = [models[m] for m in models.keys()]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(x, avgs, width, label='Average Faithfulness Score', color=colors)\n",
    "\n",
    "# Labels & Titles\n",
    "ax.set_xlabel(\"Models\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Average Faithfulness Score for Different Models\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "\n",
    "additionalY = 0.3\n",
    "ax.axvline(x = 2.5, color = '0.5')\n",
    "ax.axvline(x = 5.5, color = '0.5')\n",
    "ax.text((-0.65 + 2.5)/3, additionalY, 'Base Models', style='italic', bbox={'facecolor': '0.8', 'alpha': 1, 'pad': 5})\n",
    "ax.text((-1.3 + 2.5)/3 + 3, additionalY, 'Basic Dataset Models', style='italic', bbox={'facecolor': '0.8', 'alpha': 1, 'pad': 5})\n",
    "ax.text((-1.3 + 2.5)/3 + 6, additionalY, 'Advanced Dataset Models', style='italic', bbox={'facecolor': '0.8', 'alpha': 1, 'pad': 5})\n",
    "\n",
    "for i, m in enumerate(models.keys()):\n",
    "    ax.text(i-(0.5*width),models[m],models[m])\n",
    "\n",
    "base_model_patch = mpatches.Patch(color='#1b9e77', label='Base Model')\n",
    "basic_ds_patch = mpatches.Patch(color='#d95f02', label='Basic Dataset')\n",
    "adv_ds_patch = mpatches.Patch(color='#7570b3', label='Advanced Dataset')\n",
    "ax.legend(handles=[base_model_patch, basic_ds_patch, adv_ds_patch], loc=\"lower center\")\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"YOUR_HOME_DIR/datasets/evaluation/deepseek_COT_raft/Faithfullness_Score_AVG.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c949ad-d4ee-4d3f-a97c-b77cb4882003",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name in allResults.keys():\n",
    "    m = model_name\n",
    "    e = allResults[m]\n",
    "    scores = []\n",
    "    for r in e.test_results:\n",
    "        scores.append(r.metrics_data[0].score)\n",
    "    avg = round(statistics.fmean(scores), 2)\n",
    "    print(model_name)\n",
    "    print(e.confident_link)\n",
    "    print(\"Avg:\", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc43e72-0a6d-4863-be2a-1a0183434aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hengwen-DeepSeek-R1-Distill-Qwen-32B-q4_k_m\n",
    "#Avg: 0.94\n",
    "#https://app.confident-ai.com/project/cm8948kaq1y5bh4ws6cui71c1/evaluation/test-runs/cm8hfqlb10vnzxnpht3t9so24\n",
    "\n",
    "#meditron-7b-COT\n",
    "#Avg: 0.94\n",
    "#https://app.confident-ai.com/project/cm8948kaq1y5bh4ws6cui71c1/evaluation/test-runs/cm8hho3aj16dixnphkjp3hq7f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}


