{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dc1be-f42f-4a02-a1b7-cc8aa2ae3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hf_token = \"HUGGINGFACE TOKEN HERE\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=hf_token\n",
    "os.environ[\"HF_TOKEN\"]=hf_token\n",
    "os.environ['HF_HOME'] = 'YOUR_HOME_DIR/.cache/huggingface/'\n",
    "os.environ['TRANSFOMERS_CACHE'] = 'YOUR_HOME_DIR/.cache/huggingface/'\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5e8006-b418-4a71-9455-a258f859c900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tempfile import mkdtemp\n",
    "from warnings import filterwarnings\n",
    "from transformers import BitsAndBytesConfig\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import torch\n",
    "from llama_index.core import Settings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from peft import PeftModel\n",
    "\n",
    "EMBED_MODEL = HuggingFaceEmbedding(model_name=\"abhinand/MedEmbed-large-v0.1\")\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "lora_path=\"YOUR_HOME_DIR/LLaMA-Factory/saves/llama3-8b/noCOT/lora/epoch10_rank64_lr10e-5\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "    device_map=\"auto\",\n",
    "    #device_map={\"\": accelerator.process_index},\n",
    "    token=hf_token,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model,lora_path)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "#GEN_MODEL = HuggingFaceLLM(model=model,tokenizer_name=model_id,\n",
    "#    device_map=\"auto\",\n",
    "#    generate_kwargs={\"temperature\": 0.7, \"top_k\": 50, \"top_p\": 0.95},\n",
    "#)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "\n",
    "Settings.tokenzier = tokenizer\n",
    "#Settings.llm = model\n",
    "\n",
    "embed_dim = len(EMBED_MODEL.get_text_embedding(\"hi\"))\n",
    "print(\"Embed dim:\", embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c287e8-44f3-4a07-ba5a-698cd45795d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from docling.document_converter import DocumentConverter\n",
    "from llama_index.node_parser.docling import DoclingNodeParser\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "SOURCE = r\"YOUR_HOME_DIR/guideline_edit.md\"\n",
    "\n",
    "reader = DoclingReader()\n",
    "node_parser = MarkdownNodeParser()\n",
    "chunker = HybridChunker()\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=str(Path(\"YOUR_HOME_DIR/datasets/docling_md_vectordb.db\")),\n",
    "    dim=embed_dim,\n",
    "    overwrite=False,\n",
    ")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents=reader.load_data(SOURCE),\n",
    "    transformations=[node_parser],\n",
    "    storage_context=StorageContext.from_defaults(vector_store=vector_store),\n",
    "    embed_model=EMBED_MODEL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6656a2d-afb9-4e12-878b-a0f8d838cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html import unescape\n",
    "QUERY = \"Welche Langzeitrisiken bestehen nach chirurgischem Verschluss eines persistierenden Ductus arteriosus (PDA) im SÃ¤uglingsalter?\"\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "retrieved_docs = retriever.retrieve(QUERY)\n",
    "sources = [s.get_content(s.metadata) for s in retrieved_docs]\n",
    "sourcesStr = \"\\n\\n\".join(sources)\n",
    "\n",
    "QUERY = f\"### Input:\\n{QUERY}\\nContext:\\n{sourcesStr}\\n\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    use_cache=True,\n",
    "    device_map=\"auto\",\n",
    "    max_new_tokens=1024,\n",
    "    #max_length=16384,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    num_return_sequences=1,\n",
    "    repetition_penalty=1.5,\n",
    "    eos_token_id=[\n",
    "        128001,\n",
    "        128008,\n",
    "        128009\n",
    "      ],\n",
    "    top_p=0.9,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "Instruction = \"### Instruction:\\nYou are a medical QA bot that is tasked to answer questions as accurately as possible given excerpts of a medical guideline. If possible provide short but adequate answers to the given question based on the given context. Avoid repetitions and duplications. Do not add notes or any other information to the output except factually relevant information. If you cannot answer the question with the given information, decline generating the answer.\"\n",
    "\n",
    "QUERY = QUERY + \"\\n\\n### Response:\"\n",
    "\n",
    "QUERY = f\"<begin_of_text><start_header_id>system<end_header_id>{Instruction}<leot_id><start_header_id>user<end_header_id>{QUERY}<leot_id><start_header_id>assistant<end_header_id>\"\n",
    "\n",
    "# Output and load valid JSON\n",
    "output_dict = pipeline(QUERY)\n",
    "output = output_dict[0][\"generated_text\"][len(QUERY) :]\n",
    "\n",
    "unescapedOutput = unescape(output)\n",
    "print(\"Output:\", unescapedOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9958a930-d78e-418b-b576-554e2e94d6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How are innocent, functional, and organic heart murmurs defined and differentiated based on their underlying causes?\"\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "retrieved_docs = retriever.retrieve(query)\n",
    "print(retrieved_docs)\n",
    "\n",
    "sources = [s.get_content(s.metadata) for s in retrieved_docs]\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beee8a1-099a-41d2-9bb6-634cb3ca72a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY = \"How are innocent, functional, and organic heart murmurs defined and differentiated based on their underlying causes?\"\n",
    "result = index.as_query_engine(llm=GEN_MODEL, similarity_top_k=5, max_new_tokens=1024).query(QUERY)\n",
    "print(f\"Q: {QUERY}\\nA: {result.response.strip()}\\n{'-'*50}\\nSources:\")\n",
    "display([(n.text, n.metadata) for n in result.source_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf88d1-021f-45c2-b792-b88da9d704a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
