{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cae237-2bd9-4b9d-98a7-e2e7d62565c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.test_case import LLMTestCase, LLMTestCaseParams\n",
    "from deepeval.metrics import GEval\n",
    "import deepeval\n",
    "\n",
    "import os\n",
    "openai_token = \"YOUR OPENAI TOKEN\"\n",
    "os.environ[\"OPENAI_API_KEY\"]=openai_token\n",
    "\n",
    "deepeval.login_with_confident_api_key(\"YOUR DEEPEVAL TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc6ea2-de18-47d0-ac9a-a2963ff87816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(xss):\n",
    "    return [x for xs in xss for x in xs]\n",
    "\n",
    "def remove_thoughts_section(text):\n",
    "    return flatten([e.split(\"</think>\") for e in text.split(\"</thoughts>\")])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90297795-7e29-45f9-a4da-476a189911cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from deepeval import evaluate\n",
    "from deepeval.dataset import EvaluationDataset\n",
    "from datasets import Dataset\n",
    "\n",
    "correctness_metric = GEval(\n",
    "        name=\"Correctness\",\n",
    "        criteria=\"Determine if the 'actual output' is correct based on the 'expected output'.\",\n",
    "        evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT]\n",
    "    )\n",
    "\n",
    "usedDataset = \"deepseek_COT_raft\"\n",
    "dsBaseDir = f\"YOUR_HOME_DIR/datasets/evaluation/{usedDataset}/\"\n",
    "\n",
    "models = [f for f in os.listdir(dsBaseDir) if os.path.isdir(dsBaseDir + f)]\n",
    "\n",
    "allResults = {}\n",
    "\n",
    "IncludeThoughts = False\n",
    "for idx, m in enumerate(models):\n",
    "    print(idx, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b3846-a094-4d9d-ba74-a66fbe18e552",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "usedModel = models[8]\n",
    "\n",
    "print(\"Evaluating Correctness for\", usedModel)\n",
    "datasetDir = dsBaseDir + usedModel\n",
    "\n",
    "ds = Dataset.load_from_disk(datasetDir)\n",
    "\n",
    "testcases = []\n",
    "for e in ds:\n",
    "    if IncludeThoughts:\n",
    "        actual_output=e[\"actual_output\"]\n",
    "        expected_output=e[\"expected_output\"]\n",
    "    else:\n",
    "        actual_output=remove_thoughts_section(e[\"actual_output\"])\n",
    "        expected_output=remove_thoughts_section(e[\"expected_output\"])\n",
    "    test_case = LLMTestCase(input=e[\"input\"], actual_output=actual_output, expected_output=expected_output, retrieval_context=e[\"retrieval_context\"])\n",
    "    testcases.append(test_case)\n",
    "id = f\"new-{usedModel}-{usedDataset}-{'WithThoughts' if IncludeThoughts else 'WithoutThoughts'}\"\n",
    "dataset = EvaluationDataset(test_cases=testcases)\n",
    "\n",
    "evalRes = evaluate(dataset, metrics=[correctness_metric], throttle_value=2, identifier=id, print_results=False)\n",
    "allResults[usedModel] = evalRes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab2c90e-7a5e-4311-8eba-0c44fcf3d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Extracting values\n",
    "models = {'llama3.1-8b-instruct-fp16':0.53, 'meditron-7b-fp16':0.14, 'DeepSeek-R1-Distill-Qwen-32B':0.45, 'llama31-basicDS':0.3, 'meditron-7b-basicDS':0.23, 'qwen-32b-basicDS':0.32, 'llama31-advDS-COT':0.5, 'meditron-7b-COT':0.36, 'qwen-32b-COT-q4_K_M':0.58}\n",
    "colors = ['#1b9e77'] * 3 + ['#d95f02'] * 3 + ['#7570b3'] * 3\n",
    "\n",
    "print(models)\n",
    "avgs = [models[m] for m in models.keys()]\n",
    "\n",
    "# Plotting\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "ax.bar(x, avgs, width, label='Average Correctness Score', color=colors)\n",
    "\n",
    "# Labels & Titles\n",
    "ax.set_xlabel(\"Models\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Average Correctness Score for Different Models\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=45, ha=\"right\")\n",
    "\n",
    "for i in range(len(x)):\n",
    "    ax.text(i-(0.5*width),avgs[i],avgs[i])\n",
    "\n",
    "ax.set_ylim(0, 0.65)\n",
    "# Create a break in the y-axis\n",
    "ax.set_yticks([0, 0.2, 0.4, 0.6, 0.66])\n",
    "ax.set_yticklabels([\"0\", \"0.2\", \"0.4\", \"0.6\", \"1\"])\n",
    "ax.spines['top'].set_visible(False)\n",
    "# Add a break indicator (zigzag lines)\n",
    "ax.plot([-0.5, 8.75], [0.65, 0.66], \"k--\", lw=1)\n",
    "ax.plot([-0.5, 8.75], [0.6, 0.6], \"k--\", lw=1)\n",
    "\n",
    "base_model_patch = mpatches.Patch(color='#1b9e77', label='Base Model')\n",
    "basic_ds_patch = mpatches.Patch(color='#d95f02', label='Basic Dataset')\n",
    "adv_ds_patch = mpatches.Patch(color='#7570b3', label='Advanced Dataset')\n",
    "ax.legend(handles=[base_model_patch, basic_ds_patch, adv_ds_patch], loc=\"upper center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"YOUR_HOME_DIR/datasets/evaluation/deepseek_COT_raft/Correctness_Score_AVG.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a04a2b-e41c-4ac8-8d52-3fa57e7bfb22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
