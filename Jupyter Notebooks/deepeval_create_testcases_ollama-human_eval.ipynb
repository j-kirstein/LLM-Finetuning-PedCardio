{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343dc1be-f42f-4a02-a1b7-cc8aa2ae3ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "hf_token = \"HUGGINGFACE TOKEN HERE\"\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=hf_token\n",
    "os.environ[\"HF_TOKEN\"]=hf_token\n",
    "os.environ['HF_HOME'] = 'YOUR_HOME_DIR/.cache/huggingface/'\n",
    "os.environ['TRANSFOMERS_CACHE'] = 'YOUR_HOME_DIR/.cache/huggingface/'\n",
    "import transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bf88d1-021f-45c2-b792-b88da9d704a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_MODEL = HuggingFaceEmbedding(model_name=\"abhinand/MedEmbed-large-v0.1\")\n",
    "\n",
    "Settings.embed_model = EMBED_MODEL\n",
    "embed_dim = len(EMBED_MODEL.get_text_embedding(\"hi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c287e8-44f3-4a07-ba5a-698cd45795d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOAD VECTORE INDEX FROM DISK (NOT WORKING PROPERLY)\n",
    "\n",
    "from llama_index.core import StorageContext, VectorStoreIndex, load_index_from_storage\n",
    "from llama_index.core.storage.docstore import SimpleDocumentStore\n",
    "from llama_index.core.storage.index_store import SimpleIndexStore\n",
    "from llama_index.core.graph_stores import SimpleGraphStore\n",
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "from llama_index.readers.docling import DoclingReader\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from docling.document_converter import DocumentConverter\n",
    "from llama_index.node_parser.docling import DoclingNodeParser\n",
    "from docling.chunking import HybridChunker\n",
    "from pathlib import Path\n",
    "\n",
    "persist_dir = \"YOUR_HOME_DIR/datasets/persistent_vector_store\"\n",
    "\n",
    "vector_store = MilvusVectorStore(\n",
    "    uri=str(Path(\"YOUR_HOME_DIR/datasets/docling_md_vectordb.db\")),\n",
    "    dim=embed_dim,\n",
    "    overwrite=False,\n",
    ")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(\n",
    "    vector_store=vector_store,\n",
    "    docstore=SimpleDocumentStore.from_persist_dir(persist_dir=persist_dir),\n",
    "    graph_store=SimpleGraphStore.from_persist_dir(persist_dir=persist_dir),\n",
    "    index_store=SimpleIndexStore.from_persist_dir(persist_dir=persist_dir),\n",
    ")\n",
    "\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b98cfe7-1145-487a-9406-053a07a5bf11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=5)\n",
    "\n",
    "def generate_response(question, model_name=\"qwen-32b-COT\"):\n",
    "    retrieved_docs = retriever.retrieve(question)\n",
    "    print(f\"Retrieved {len(retrieved_docs)} docs\")\n",
    "    sources = [s.get_content(s.metadata) for s in retrieved_docs]\n",
    "    sourcesStr = \"\\n\\n\".join(sources)\n",
    "    \n",
    "    QUERY = f\"### Input:\\n{question}\\nContext:\\n{sourcesStr}\\n\"\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model_name,\n",
    "        \"prompt\": QUERY,\n",
    "        \"stream\": True,\n",
    "        \"options\": {\"num_predict\":4096}\n",
    "    }\n",
    "    url = \"http://leinevmgpu001.mh-hannover.local:11434/api/generate\"\n",
    "    #for i in range(5):\n",
    "    #    try:\n",
    "    response = requests.post(url, json=data, timeout=120)\n",
    "    con_text = \"\"\n",
    "    for l in response.text.split(\"\\n\"):\n",
    "        try:\n",
    "            obj = json.loads(l)\n",
    "        except:\n",
    "            #print(f\"Failed at '{l}'\")\n",
    "            continue    \n",
    "        if \"done\" in obj:\n",
    "            if obj[\"done\"] == True:\n",
    "                text = con_text\n",
    "            else:\n",
    "                con_text += obj[\"response\"]\n",
    "    #        break\n",
    "    #    except:\n",
    "    #        print(f\"Errored {i}\")\n",
    "    return text, sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc02b2c-b864-46a1-917d-6144a1869f49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelNames = [\"llama3.1:8b-instruct-fp16\"]\n",
    "\n",
    "for usedModel in modelNames:\n",
    "    print(f\"Generating for model {usedModel}\")\n",
    "    usedDataset = \"deepseek_COT_raft\"\n",
    "    savePath = f\"YOUR_HOME_DIR/datasets/evaluation/human_eval/{usedModel.replace(':', '-').replace('/', '-')}\"\n",
    "    \n",
    "    from pyarrow.parquet import ParquetDataset\n",
    "    import time\n",
    "    dataset = ParquetDataset(f\"YOUR_HOME_DIR/datasets/evaluation/human_eval/base.parquet\")\n",
    "    pds = dataset.read().to_pandas()\n",
    "    \n",
    "    test_cases = []\n",
    "    for idx, row in pds.iterrows():\n",
    "        d = row.to_dict()\n",
    "        question = d[\"question\"]\n",
    "        output = d[\"answer\"]\n",
    "        try:\n",
    "            generated_output, used_sources = generate_response(question, model_name=usedModel)\n",
    "            case = {}\n",
    "            case[\"input\"] = question\n",
    "            case[\"expected_output\"] = output\n",
    "            case[\"actual_output\"] = generated_output\n",
    "            case[\"retrieval_context\"] = used_sources\n",
    "            test_cases.append(case)\n",
    "        except Exception as e:\n",
    "            print(\"error at \", idx)\n",
    "    \n",
    "    print(f\"Saving to {savePath}\")\n",
    "    \n",
    "    for e in test_cases:\n",
    "        for k in e.keys():\n",
    "            print(k)\n",
    "            print(e[k])\n",
    "        print(\"#\" * 50)\n",
    "    \n",
    "    from datasets import Dataset\n",
    "    \n",
    "    def saveDS():\n",
    "        for tc in test_cases:\n",
    "            yield tc\n",
    "    \n",
    "    tc_dataset = Dataset.from_generator(saveDS)\n",
    "    tc_dataset.save_to_disk(savePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abb698-15ac-4bd9-8b74-e36126a55522",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2018f51-e365-41a4-8e8f-99bc517f7822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Define the base URL for the Ollama server\n",
    "hostname = \"leinevmgpu001.mh-hannover.local\"\n",
    "url = f\"http://{hostname}:11434/api/tags\"  # Adjust the URL if needed based on your server setup\n",
    "\n",
    "try:\n",
    "    # Send a GET request to the server to fetch installed models\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful (status code 200)\n",
    "    if response.status_code == 200:\n",
    "        models = response.json()  # Assuming the server responds with a JSON list of models\n",
    "        print(\"Installed models on the Ollama server:\")\n",
    "        print([m[\"name\"] for m in models[\"models\"]])\n",
    "    else:\n",
    "        print(f\"Failed to fetch models. HTTP Status Code: {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daac516c-1d83-4e61-a6d6-5d182ee1aa63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
